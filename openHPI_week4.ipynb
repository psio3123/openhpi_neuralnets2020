{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openHPI_week4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMlFAd0E5fWSNmJ4VWZPndY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_vdqfgmAXcJ",
        "colab_type": "text"
      },
      "source": [
        "Install HPI tool and get trainings-data, init keras and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkxaXvYST4Zl",
        "colab_type": "code",
        "outputId": "1c2b419c-cae1-477f-ab06-869fa67aeac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# Befehl für Google Colab für Tensorflow 2\n",
        "%tensorflow_version 2.x \n",
        "\n",
        "# ohne Ausrufezeichen bei Ausführung im lokalen Notebook\n",
        "!pip install --upgrade deeplearning2020\n",
        "from deeplearning2020.datasets import ImageWoof\n",
        "\n",
        "#!pip install tf-nightly-gpu\n",
        "\n",
        "train_data, test_data, classes = ImageWoof.load_data()\n",
        "\n",
        "# TensorFlow ≥2.0 wird benötigt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "else:\n",
        "  print(\"GPU ok\")\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Activation, Input, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "!pip install --upgrade deeplearning2020\n",
        "from deeplearning2020 import helpers\n",
        "\n",
        "# jupyters notebook Befehl zum direkten Anzeigen von Matplotlib Diagrammen\n",
        "%matplotlib inline\n",
        "\n",
        "#get helpers\n",
        "from deeplearning2020 import helpers\n",
        "\n",
        "#show sample data\n",
        "#helpers.plot_images(train_data_orig.take(9), classes)\n",
        "\n",
        "#def preprocess(image, label):\n",
        "#    resized_image = tf.image.resize(image, [300, 300])\n",
        "#    return resized_image, label\n",
        "\n",
        "## Festlegung der Batch Größe für die Datenvorbereitung\n",
        "#batch_size = 32 \n",
        "\n",
        "## Durchmischen der Trainingsdaten, dass nicht mit sortierten Bildern trainiert wird \n",
        "#train_data = train_data.shuffle(1000) \n",
        "#\n",
        "#print('shape des Trainigsdatensatzes vor dem preprocessing: ', train_data)\n",
        "#\n",
        "#train_data = train_data.map(preprocess) \\\n",
        "#  .batch(batch_size).prefetch(1)          \n",
        "#test_data = test_data.map(preprocess) \\\n",
        "#  .batch(batch_size).prefetch(1)\n",
        "\n",
        "#print('shape des Traingingsdatensatzes nach dem preprocessing: ', train_data)\n",
        "\n",
        "#show refined data\n",
        "#helpers.plot_images(train_data.unbatch().take(9), classes)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: deeplearning2020 in /usr/local/lib/python3.6/dist-packages (0.4.21)\n",
            "Requirement already satisfied, skipping upgrade: kerasltisubmission>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from deeplearning2020) (0.4.9)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (3.0.4)\n",
            "/root/.keras/datasets/imagewoof2-320/train\n",
            "Loaded 9025 images\n",
            "/root/.keras/datasets/imagewoof2-320/val\n",
            "Loaded 3929 images\n",
            "WARNING:tensorflow:From <ipython-input-1-7f443555b665>:16: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU ok\n",
            "Sun Apr 12 21:56:23 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    32W / 250W |    373MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already up-to-date: deeplearning2020 in /usr/local/lib/python3.6/dist-packages (0.4.21)\n",
            "Requirement already satisfied, skipping upgrade: kerasltisubmission>=0.4.9 in /usr/local/lib/python3.6/dist-packages (from deeplearning2020) (0.4.9)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yTB94CwMhET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "59541e0c-4a3c-4142-9970-b4b609c339b8"
      },
      "source": [
        "img_sizeX = 300\n",
        "img_sizeY = 300\n",
        "\n",
        "n_train = 9025\n",
        "n_test = 3929\n",
        "\n",
        "import typing\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "def myPreprocess(image, label):\n",
        "  resized_image = tf.image.resize(image, [img_sizeX, img_sizeY])\n",
        "  return resized_image, label\n",
        "\n",
        "def myDatasetToNdarray(\n",
        "    train_data: \"DatasetV1Adapter\", test_data: \"DatasetV1Adapter\"\n",
        ") -> typing.Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "\n",
        "    preprocessed_train_data = train_data.take(n_train).map(myPreprocess).prefetch(2)\n",
        "    preprocessed_test_data = test_data.take(n_test).map(myPreprocess).prefetch(2)\n",
        "\n",
        "    print(\"Creating numpy arrays for image data...\")\n",
        "\n",
        "    x = np.empty([n_train, img_sizeX, img_sizeY, 3], dtype=\"float32\")\n",
        "    y = np.empty([n_train], dtype=\"int64\")\n",
        "    x_test = np.empty([n_test, img_sizeX, img_sizeY, 3], dtype=\"float32\")\n",
        "    y_test = np.empty([n_test], dtype=\"int64\")\n",
        "\n",
        "    print(\"Train data: conversion to numpy...\")\n",
        "\n",
        "    i = 0;\n",
        "    for elem in preprocessed_train_data:\n",
        "        x[i] = elem[0].numpy()\n",
        "        y[i] = elem[1].numpy()\n",
        "        i += 1\n",
        "        if i % 100 == 0 or i == n_train: print('\\r', i, end='')\n",
        "\n",
        "    print(\"\\nTest data: conversion to numpy...\") \n",
        "\n",
        "    i = 0;\n",
        "    for elem in preprocessed_test_data:\n",
        "        x_test[i] = elem[0].numpy()\n",
        "        y_test[i] = elem[1].numpy()\n",
        "        i += 1\n",
        "        if i % 100 == 0 or i == n_test: print('\\r', i, end='')\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "    return x, y, x_test, y_test\n",
        "\n",
        "# Durchmischen der Trainingsdaten, dass nicht mit sortierten Bildern trainiert wird \n",
        "train_data = train_data.shuffle(10000)\n",
        "\n",
        "# convert tf.dataset to numpy array\n",
        "X, y, X_test, y_test = myDatasetToNdarray(train_data, test_data)\n",
        "\n",
        "#memory saving\n",
        "del train_data\n",
        "del test_data\n",
        "\n",
        "\n",
        "n_classes = len(classes)\n",
        "dataset_size = len(X)\n",
        "batch_size = 32"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating numpy arrays for image data...\n",
            "Train data: conversion to numpy...\n",
            " 9025\n",
            "Test data: conversion to numpy...\n",
            " 3929\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-swpdfbjFKby",
        "colab_type": "text"
      },
      "source": [
        "Defining the model via function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7RtiKklFO_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns a compiled keras model with the parameters given\n",
        "def wrap_model(\n",
        "    learning_rate=0.001, \n",
        "    momentum=0.9,\n",
        "    decay=0.001, \n",
        "    dense_neurons=1000,\n",
        "    n_filters=32,\n",
        "    first_kernel_size=(7,7)):\n",
        "\n",
        "  activation='elu'\n",
        "\n",
        "  input_layer = Input(shape=(300, 300, 3))\n",
        "\n",
        "  model = Conv2D(\n",
        "      filters=n_filters, \n",
        "      kernel_size=first_kernel_size, \n",
        "      activation=activation\n",
        "  )(input_layer)\n",
        "  model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "  for i in range(2, 5):\n",
        "    model = Conv2D(\n",
        "        filters = i * n_filters, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activation\n",
        "      )(model)\n",
        "    model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "  model = Conv2D(\n",
        "      filters = 5 * n_filters, \n",
        "      kernel_size=(3,3), \n",
        "      activation=activation, \n",
        "      padding='same'\n",
        "    )(model)\n",
        "  model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "  model = Flatten()(model)\n",
        "  model = Dense(dense_neurons, activation=activation)(model)\n",
        "  model = Dense(dense_neurons / 2, activation='tanh')(model)\n",
        "\n",
        "  output = Dense(n_classes, activation=\"softmax\")(model)\n",
        "\n",
        "  CNN_model = Model(input_layer, output)\n",
        "\n",
        "  optimizer = keras.optimizers.SGD(\n",
        "      lr=learning_rate, \n",
        "      momentum=momentum, \n",
        "      decay=decay\n",
        "  )\n",
        "  CNN_model.compile(\n",
        "      loss=\"sparse_categorical_crossentropy\", \n",
        "      optimizer=optimizer,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "  \n",
        "  return CNN_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLsJSNKBXdrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "20a6567d-46b2-40b1-a8f7-b3f5177b00dd"
      },
      "source": [
        "CNN_model = wrap_model()\n",
        "CNN_model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 294, 294, 32)      4736      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 147, 147, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 145, 145, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 70, 70, 96)        55392     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 128)       110720    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 160)       184480    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 160)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10240)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              10241000  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               500500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 11,120,334\n",
            "Trainable params: 11,120,334\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcIXKvNdFkzf",
        "colab_type": "text"
      },
      "source": [
        "Randomized Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f288Piv1FnBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params_random = {\n",
        "    'learning_rate':[0.01, 0.03], \n",
        "    'momentum':[0.9, 0.99, 0.999],\n",
        "    'decay':[0.1, 0.01, 0.001], \n",
        "    'dense_neurons':[200, 500],# 1000, 2000],\n",
        "    'n_filters':[16, 32, 64],\n",
        "    'first_kernel_size':[(3,3), (5,5), (7,7)]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F83Z3CL4FuZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b812e20d-571b-4f3f-8042-f889dc85d65a"
      },
      "source": [
        "keras_regressor = tf.keras.wrappers.scikit_learn.KerasRegressor(wrap_model)\n",
        "random_search = RandomizedSearchCV(keras_regressor, params_random, n_iter=20, verbose=1)\n",
        "random_search.fit(X, y, epochs=12, validation_data=(X_test, y_test))\n",
        "print(\"Best Parameters: \", random_search.best_params_)\n",
        "best_model = random_search.best_estimator_\n",
        "from deeplearning2020 import Submission\n",
        "Submission('4d712a6d0ae14f2a395c992ad3627476', '3', best_model).submit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "187/226 [=======================>......] - ETA: 2s - loss: 5.1879 - accuracy: 0.1066"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}