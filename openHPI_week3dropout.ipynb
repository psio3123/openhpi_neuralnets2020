{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openHPI_week3dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPIg+nU5uooQTUEINDlCqJ+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9wK8PkaT33p",
        "colab_type": "text"
      },
      "source": [
        "Install HPI tool and get trainings-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkxaXvYST4Zl",
        "colab_type": "code",
        "outputId": "d16a490b-fcd0-4ef4-d671-ae6274ddb453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "%tensorflow_version 2.x # Befehl für Google Colab für Tensorflow 2\n",
        "\n",
        "# ohne Ausrufezeichen bei Ausführung im lokalen Notebook\n",
        "!pip install --upgrade deeplearning2020\n",
        "from deeplearning2020.datasets import ImageWoof\n",
        "\n",
        "train_data, test_data, classes = ImageWoof.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x # Befehl für Google Colab für Tensorflow 2`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Collecting deeplearning2020\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/7f/6fee39d7590f4ae20a976131b1920d56a3dee138c208dfcb3959cd8c5275/deeplearning2020-0.4.21.tar.gz\n",
            "Collecting kerasltisubmission>=0.4.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/56/0b6adef8e6f5d89e9daa68e03d00850509f1553ce6303c0a49d7c619dd26/kerasltisubmission-0.4.9.tar.gz (392kB)\n",
            "\u001b[K     |████████████████████████████████| 399kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2020.4.5.1)\n",
            "Building wheels for collected packages: deeplearning2020, kerasltisubmission\n",
            "  Building wheel for deeplearning2020 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplearning2020: filename=deeplearning2020-0.4.21-py2.py3-none-any.whl size=8548 sha256=50e0affea7431ec4a943fd5cbca84b7c42159ca94cb514bd83544cd3fca8dd79\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/c2/8a/f9f03fc839999f1fe9d5e5a9d2c97cdd5cb8329f61f82ea2c9\n",
            "  Building wheel for kerasltisubmission (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasltisubmission: filename=kerasltisubmission-0.4.9-py2.py3-none-any.whl size=8867 sha256=da096ad17801a40eaced6a1304adffba4a5e7a88e31934a14fa310ed9e909ed8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/61/f7/09171376b25408ae21b58e98c9fbf2eb924f676bb77659f983\n",
            "Successfully built deeplearning2020 kerasltisubmission\n",
            "Installing collected packages: kerasltisubmission, deeplearning2020\n",
            "Successfully installed deeplearning2020-0.4.21 kerasltisubmission-0.4.9\n",
            "Downloading data from https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-320.tgz\n",
            "328294400/328288506 [==============================] - 3s 0us/step\n",
            "/root/.keras/datasets/imagewoof2-320/train\n",
            "Loaded 9025 images\n",
            "/root/.keras/datasets/imagewoof2-320/val\n",
            "Loaded 3929 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFh1v0XCYWMF",
        "colab_type": "text"
      },
      "source": [
        "Init *Tensorflow* and stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHiZFtvnYaA2",
        "colab_type": "code",
        "outputId": "8b592d4e-7313-47f8-e4c6-be27a9bb46a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# TensorFlow ≥2.0 wird benötigt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "else:\n",
        "  print(\"GPU ok\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Activation, Input, \\\n",
        "  Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from deeplearning2020 import helpers\n",
        "\n",
        "# jupyters notebook Befehl zum direkten Anzeigen von Matplotlib Diagrammen\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-3114b4965615>:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt3b3CO7c6sE",
        "colab_type": "text"
      },
      "source": [
        "Define the preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTcyXi24c9GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(image, label):\n",
        "    resized_image = tf.image.resize(image, [300, 300])\n",
        "    return resized_image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mdjoH1gzVl",
        "colab_type": "text"
      },
      "source": [
        "Resize the pictures and prepare batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXbNr0pNgxmc",
        "colab_type": "code",
        "outputId": "ad4bff75-e066-466f-bf81-3015c19cae44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Festlegung der Batch Größe für die Datenvorbereitung\n",
        "batch_size = 32 \n",
        "\n",
        "# Durchmischen der Trainingsdaten, dass nicht mit sortierten Bildern trainiert wird \n",
        "train_data = train_data.shuffle(1000) \n",
        "\n",
        "print('shape des Trainigsdatensatzes vor dem preprocessing: ', train_data)\n",
        "\n",
        "train_data = train_data.map(preprocess) \\\n",
        "  .batch(batch_size).prefetch(1)          \n",
        "test_data = test_data.map(preprocess) \\\n",
        "  .batch(batch_size).prefetch(1)\n",
        "\n",
        "print('shape des Traingingsdatensatzes nach dem preprocessing: ', train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape des Trainigsdatensatzes vor dem preprocessing:  <ShuffleDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
            "shape des Traingingsdatensatzes nach dem preprocessing:  <PrefetchDataset shapes: ((None, 300, 300, 3), (None,)), types: (tf.float32, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW9OEuzxSAHY",
        "colab_type": "text"
      },
      "source": [
        "Show train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-250b84SCYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helpers.plot_images(train_data.unbatch().take(9), classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsvC45-qeT95",
        "colab_type": "text"
      },
      "source": [
        "Define the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBEVsODPerZ3",
        "colab_type": "code",
        "outputId": "0c5ef82b-2b4f-494a-a8bf-da62b345fc7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "\n",
        "# model\n",
        "learning_rate=0.01\n",
        "momentum=0.9\n",
        "decay=0.001\n",
        "dense_neurons=800\n",
        "n_filters=32\n",
        "first_kernel_size=(7,7)\n",
        "\n",
        "n_classes=len(classes)\n",
        "\n",
        "activation='elu'\n",
        "\n",
        "# Inputgröße muss 300x300 Pixel mit 3 RGB Farben betragen\n",
        "input_layer = Input(shape=(300, 300, 3))\n",
        "input = BatchNormalization(axis=[1,2])(input_layer)\n",
        "\n",
        "# Convolutional Neural Network\n",
        "# Entry Convolutional Layers (with pooling)\n",
        "model = Conv2D(filters=n_filters, kernel_size=first_kernel_size, activation=activation )(input)\n",
        "model = BatchNormalization(axis=[1,2])(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "# 4 Conv layer (with pooling)\n",
        "for i in range(2, 5):\n",
        "  model = Conv2D(filters = i * n_filters, kernel_size=(3,3), activation=activation)(model)\n",
        "  #model = Conv2D(filters = i * n_filters, kernel_size=(3,3), activation=activation, padding='same')(model)\n",
        "  model = BatchNormalization(axis=[1,2])(model)\n",
        "  model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "# final conv & pooling\n",
        "model = Conv2D(filters = 5 * n_filters, kernel_size=(3,3), activation=activation, padding='same')(model)\n",
        "model = BatchNormalization(axis=[1,2])(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "# Fully-Connected-Classifier\n",
        "model = Flatten()(model)\n",
        "model = Dense(dense_neurons, activation=activation)(model)\n",
        "model = BatchNormalization()(model)\n",
        "model = Dropout(0.5)(model)\n",
        "\n",
        "# Fully connected layer\n",
        "model = Dense(dense_neurons / 2, activation='tanh')(model)\n",
        "model = BatchNormalization()(model)\n",
        "model = Dropout(0.5)(model)\n",
        "\n",
        "# Output Layer\n",
        "output = Dense(n_classes, activation=\"softmax\")(model)\n",
        "\n",
        "#Full model\n",
        "CNN_model = Model(input_layer, output)\n",
        "\n",
        "# Kompilieren des Modells\n",
        "## Stochastic Gradient Descent -> langsamer aber genauer\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay)\n",
        "## Adam -> schneller, für schnellere aber nicht 100% Ergebnisse\n",
        "#optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "CNN_model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
        "CNN_model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 294, 294, 32)      4736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 294, 294, 32)      345744    \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 147, 147, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 145, 145, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 145, 145, 64)      84100     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 72, 72, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 70, 70, 96)        55392     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 70, 70, 96)        19600     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 128)       110720    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 33, 33, 128)       4356      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 160)       184480    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 160)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 160)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10240)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 800)               8192800   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 800)               3200      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 400)               320400    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 400)               1600      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                4010      \n",
            "=================================================================\n",
            "Total params: 9,350,658\n",
            "Trainable params: 9,120,846\n",
            "Non-trainable params: 229,812\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmJTnsHji1jS",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlOKArUfi2Hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training des Modells\n",
        "history = CNN_model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    validation_data=test_data\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXQGixnkqwf",
        "colab_type": "text"
      },
      "source": [
        "Tranings-Ergebnis anzeigen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kK2YbhOkrli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "helpers.plot_history('MyCNN', history, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewEIeZBuWcH",
        "colab_type": "text"
      },
      "source": [
        "Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkxmTvrluWu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from deeplearning2020 import Submission\n",
        "#Submission('4d712a6d0ae14f2a395c992ad3627476', '3', CNN_model).submit()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}